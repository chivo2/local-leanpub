#    An&#225;lisis de la regresi&#243;n  





El an&#225;lisis  de la regresi&#243;n   es un proceso estad &#237; stico para estimar las relaciones entre variables. Incluye muchas t&#233;cnicas para el modelado y an&#225;lisis  de diversas variables, cuando la atenci&#243;n se centra en la relaci&#243;n entre una variable dependiente y una o m&#225;s variables independientes (o predictoras). M&#225;s espec&#237;ficamente, el an&#225;lisis  de regresi&#243;n   ayuda a entender como el valor de la variable dependiente varia al cambiar el valor de una de las variables independientes, manteniendo el valor de las otras variables independientes fijas. M&#225;s com&#250;nmente, el an&#225;lisis  de regresi&#243;n   estima la esperanza condicional de la variable dependiente dadas las variables independientes - es decir, el valor promedio de la variable dependiente cuando se fijan las variables independientes. Con menor frecuencia, la atenci&#243;n  se centra en un cuantil, u otro par&#225;metro de localizaci&#243;n de la distribuci&#243;n condicional de la variable dependiente dadas las variables independientes. En todos los casos, el objetivo es la estimacion de una funcion de las variables independientes llamada la funci&#243;n de regresi&#243;n  . En el an&#225;lisis  de regresi&#243;n  , tambi&#233;n es de inter&#233;s para caracterizar la variaci&#243;n de la variable dependiente en torno a la funci&#243;n de regresi&#243;n   que puede ser descrito por una distribuci&#243;n de probabilidad.

El an&#225;lisis  de regresi&#243;n   es ampliamente utilizado para la predicci&#243;n y previsi&#243;n, donde su uso tiene superposici&#243;n sustancial en el campo de aprendizaje autom&#225;tico. El an&#225;lisis  de regresi&#243;n   se utiliza tambi&#233;n para comprender que cuales de las variables independientes est&#225;n relacionadas con la variable dependiente, y explorar las formas de estas relaciones. En circunstancias limitadas, el an&#225;lisis  de regresi&#243;n   puede utilizarse para inferir relaciones causales entre las variables independientes y dependientes. Sin embargo, esto puede llevar a ilusiones o falsas relaciones, por lo que se recomienda precaucion,1 por ejemplo, la correlaci&#243;n no implica causalidad.

Se han desarrollado muchas t&#233;cnicas para llevar a cabo an&#225;lisis  de regresi&#243;n  . M&#233;todos familiares tales como regresi&#243;n   lineal y ordinaria de minimos cuadrados de regresi&#243;n   son param&#233;trica, en que la funci&#243;n de regresi&#243;n   se define en t&#233;rminos de un numero finito de desconocidos par&#225;metros que se estiman a partir de los datos. regresi&#243;n   no param&#233;trica se refiere a las t&#233;cnicas que permiten que la funci&#243;n de regresi&#243;n   mienta en un conjunto especifico de funciones, que puede ser de dimensi&#243;n infinita.

El desempe&#241;o de los m&#233;todos de an&#225;lisis  de regresi&#243;n   en la pr&#225;ctica depende de la forma del proceso de generaci&#243;n de datos, y como se relaciona con el metodo de regresi&#243;n   que se utiliza. Dado que la forma verdadera del proceso de generaci&#243;n de datos generalmente no se conoce, el an&#225;lisis  de regresi&#243;n   depende a menudo hasta cierto punto de hacer suposiciones acerca de este proceso. Estos supuestos son a veces comprobable si una cantidad suficiente de datos est&#225; disponible. Los modelos de regresi&#243;n   para la predicciamente, aunque pueden no funcionar de manera &#243;ptima. Sin embargo, en muchas aplicaciones, sobre todo con peque&#241;os efectos o las cuestiones de causalidad sobre la base de los datos de observaci&#243;n, m&#233;todos de regresi&#243;n   pueden dar resultados enga&#241;osos.

**Historia**

La primera forma de regresi&#243;n   fue el m&#233;todo de m&#237;nimos cuadrados, que fue publicado por Legendre en 1805 y por Gauss en 1809 Legendre y Gauss tanto aplicaron el m&#233;todo para el problema de determinar, a partir de observaciones astron&#243;micas, la orbitas de los cuerpos sobre el Sol (la mayor&#237;a de los cometas, sino tambi&#233;n m&#225;s tarde el entonces reci&#233;n descubiertos planetas menores). Gauss p&#250;blico un desarrollo posterior de la teor&#237;a de los m&#237;nimos cuadrados en 1821 incluyendo una versi&#243;n del teorema de Gauss-Markov.

El t&#233;rmino "regresi&#243;n  " fue acu&#241;ado por Francis Galton en el siglo XIX para describir un fen&#243;meno biol&#243;gico. El fen&#243;meno fue que las alturas de los descendientes de ancestros altos tienden a regresar hacia abajo, hacia un promedio normal (un fen&#243;meno conocido como regresi&#243;n   hacia la media ). Para Galton, la regresi&#243;n   solo ten&#237;a este significado biol&#243;gico, pero su trabajo se extendi&#243; m&#225;s tarde por Udny Yule y Karl Pearson a un contexto estad &#237; stico  m&#225;s general. En la obra de Yule y Pearson, la distribuci&#243;n conjunta de la respuesta y las variables explicativas se supone que es de Gauss. Esta suposici&#243;n se vio debilitada por RA Fisher en sus obras de 1922 y 1925 Fisher supone que la distribuci&#243;n condicional de la variable de respuesta es de Gauss, pero la distribuci&#243;n conjunta no es necesario. A este respecto, la asunci&#243;n de Fisher esta m&#225;s cerca de la formulaci&#243;n de Gauss de 1821.

En los a&#241;os 1950 y 1960, los economistas utilizan calculadoras electromec&#225;nicas para calcular regresi&#243;n  es. Antes de 1970, a veces tardaba hasta 24 horas para recibir el resultado de una regresi&#243;n  .

Los m&#233;todos de regresi&#243;n   siguen siendo un &#225;rea de investigaci&#243;n activa. En las ultimas d&#233;cadas, los nuevos m&#233;todos han sido desarrollados para la regresi&#243;n   robusta, la regresi&#243;n   que implica respuestas correlacionadas, tales como series de tiempo y las curvas de crecimiento, regresi&#243;n   en la que los predictores o variables de respuesta son curvas, im&#225;genes, gr&#225;ficos y otros objetos de datos complejos, los m&#233;todos de regresi&#243;n   Aceptar varios tipos de datos faltantes, la regresi&#243;n   no param&#233;trica, bayesianos m&#233;todos de regresi&#243;n  , regresi&#243;n   en el que las variables de predicci&#243;n se miden con error, regresi&#243;n   con m&#225;s variables predictoras que las observaciones y la inferencia causal con la regresi&#243;n  .
