# Redes de Neuronas Artificiales

Se puede considerar que las primeras investigaciones en redes de neuronas artificiales fueron a principios del siglo XIX, como por ejemplo los estudios escritos por Freud en el periodo del presicoanalisis. Rusell [Rusell, 1931] hizo la primera implementaci&#243;n de una Red de Neuronas artificiales con un dispositivo hidr&#225;ulico descrito por El. En el siglo pasado a partir de los a&#241;os 40 el estudio de redes de neuronas artificiales ha crecido, debido al avance cient&#237;fico y a la creciente capacidad de los ordenadores actuales.

En las d&#233;cadas 40 y 50 se puede destacar a Warren McCulloch y Walter Pitts, llevaron a cabo un modelo matem&#225;tico de una Red de Neuronas Artificiales. Su modelo parte de la idea de que las neuronas operan mediante impulsos binarios. Su modelo utiliza una funci&#243;n de paso por umbral. Esta idea se utiliza en modelos posteriores como en las redes de neuronas artificiales discretas de Hopfiels y la memoria asociativa bidireccional discreta. Lo mejor y novedoso del modelo radica en la capacidad de aprendizaje. Estos trabajos se pueden leer en el libro Embodiments of Mind . En la misma d&#233;cada Donald Hebb propuso un algoritmo matem&#225;tico de aprendizaje. Sus trabajos se pueden estudiar en su libro Organization of Behavior, se puede leer un paradigma de aprendizaje que se conoce por su nombre Aprendizaje Hebbiano. M&#225;s adelante en 1951 Marvin Minsky realiz&#243; trabajos pr&#225;cticos en redes de neuronas artificiales. A partir de estos trabajos McCulloch y Pitts, Minsky y Edmons construyeron una maquina con tubos, motores y rel&#233;s, que pudo modelizar el comportamiento de una rata buscando comida en un laberinto, dicha maquina ten&#237;a 40 neuronas cuyas conexiones se ajustaban seg&#250;n los sucesos y tareas que ocurrIan. Posteriormente Albert Uttley  desarrollo una maquina te&#243;rica compuesta de elementos de proceso. Utilizando la medida de entropIa de Shannon, creo un separador lineal que ajustaba sus par&#225;metros de entrada. Se utilizaron para el reconocimiento adaptativo de patrones y para el estudio de fen&#243;menos atmosf&#233;ricos. En la Ultima parte de la d&#233;cada 50, en 1957 Frank Rosenblatt al modelo de Warren McCulloch le a&#241;adi&#243; el aprendizaje , a este modelo se le conoce por el Perceptron. Primero propuso un modelo de dos niveles, que ajustaba los pesos de las conexiones entre los niveles de entrada y salida, seg&#250;n el error entre la salida deseada y la obtenida. Intento extender su modelo con una capa intermedia, capa oculta, pero no encontr√≥ &#243; un algoritmo que permitiera entrenar la red cuando ten&#237;a tres capas. Se pueden leer estos trabajos en su libro Principles of Neurodynamics.

Bernard Widrow  propuso una Red Neuronal similar al Perceptron, llamada Adaptative Linear Element o ADALINE. Aunque su modelo de dos capas tambi&#233;n ajusta el error entre el valor esperado y el obtenido, sus diferencias son peque&#241;as pero van dirigidos a aplicaciones distintas, Widrow y Marcian Off demostraron matem&#225;ticamente que el error entre la salida deseada y la obtenida puede ser tan peque&#241;o como se quiera en determinadas circunstancias. El Perceptron y ADALINE al no poseer capa oculta no pueden resolver el problema de la separabilidad lineal. En la d&#233;cada de los 60 Steinbuch propuso m&#233;todos de codificaci&#243;n de informaci&#243;n en redes de neuronas [Steinbuch, 1961]. La redes dise&#241;ados por Steinbuch se utilizaron en el reconocimiento de escritura a mano, para el control en procesos de producci&#243;n y en diagnostico en m&#225;quinas, para detectar fallos. Tambi&#233;n cabe destacar a Stephen Grossberg. Trabaj&#243; tanto en fen&#243;menos psicol&#243;gicos y biol&#243;gicos de procesamiento humano de la informaci&#243;n y realiz&#243; una teorIa unificando los dos [Grossberg, 1964]. En sus trabajos se encuentran importantes an&#225;lisis matem&#225;ticos, se obtenIa una acceso a la informaci&#243;n en tiempo real mientras se realizaban los c&#225;lculos. Grossberg perteneci&#243; al grupo de investigaci&#243;n Center for Adaptative Systems de la Universidad de Boston. Este grupo investig&#243; fundamentalmente temas relacionados con las redes de neuronas. A finales de los 60 y principios de los 70 se puede destacar a Shun-Ichi Amari que utiliz&#243; modelos matem&#225;ticos de redes de neuronas con redes de neuronas biol&#243;gicas. Para el problema de la asignaci&#243;n de crditos que era irresoluble, encontr&#243; una soluci&#243;n. Entre sus trabajos se encuentra el estudio de redes de neuronas artificiales, din&#225;micas y aleatoriamente conectadas, algoritmos de aprendizaje competitivo y el an&#225;lisis matem&#225;tico
de memorias asociativas.

En la d&#233;cada de los 70 James Anderson, desarrollo un modelo de memoria basado en la asociaci&#243;n de actividades de las sinapsis de una neurona y un modelo de memoria asociativa lineal, seg&#250;n el planteamiento de Hebb. Para reducir el error sustituyo la funci&#243;n lineal umbral por otra en rampa, propuso un nuevo modelo llamado Brain state in a box. Investigadores del Departamento de Maquinas Inteligentes de la Universidad de Edimburgo, en el a&#241;o 1968, encontraron la relaci&#243;n entre memorias asociativas y hologramas, los dos procesos pueden encontrar un patr&#243;n con muy pocos datos. Longuet y Higgins propusieron un sistema de ecuaciones codificadas para guardar y recuperar secuencias de se&#241;ales. Posteriormente, Willshaw y Buneman y Longuet y Higgins como proceso de la memoria humana propusieron los principios. A partir de estos trabajos la Universidad de Edimburgo propuso un modelo temporal de red de neuronas artificial que llamo Holophone un desarrollo que almacena se&#241;ales de entrada y puede obtener una se&#241;al completa a partir de una parte de ella. Kunihiko Fukushima realiz&#243; diversos trabajos en redes de neuronas artificiales, encontr&#243; para sistemas de visi&#243;n algunos modelos espaciales, espacio-temporales y el cerebro [Fukushima, 1970]. Utilizo un primer modelo de red multicapa para la visi&#243;n. Entre sus trabajos destacan el COGNITRON asI como la versi&#243;n llamada NEOCOGNITRON TambiEn cabe destacar A. Harry Klopf que realiz&#243; trabajos acerca de la psicolog&#237;a de la mente y la biologIa del cerebro .Tambi&#233;n son de gran importancia sus teor&#237;as sobre el sistema adaptativo.

En la d&#233;cada de los 70 y 80 destac&#243; Teuvo Kohonen, sus trabajos  se realizaron acerca de memorias asociativas y matrices de correlaci&#243;n, as&#237; como los trabajos de Anderson. Kohonen junto Ruohonen avanzaron en el modelo de memoria asociativa lineal, que necesitaba vectores linealmente independientes para obtener buenos resultados, en uno que buscaba las _optimas entre vectores linealmente dependientes, llamado Asociador Optimo de Memoria Lineal (OLAM) [Kohonen, 1977]. TambiEn realiz&#243; investigaciones en m&#233;todos de aprendizaje y desarrollo el LVQ (Learning Vector Quantization), un sistema de aprendizaje competitivo.

A principios de los 70, el premio Nobel Leon Cooper y Charles Elbaum empezaron a trabajar en Redes de Neuronas artificiales. Mediante la formaci&#243;n del grupo Nestor Associatess desarrollaron patentes y exportaron comercialmente algunas redes de neuronas artificiales. Desarrollaron la red RCE (Reduced Coulomb Energy)  asI como el sistema de aprendizaje NSL (Nestor Learning System). Terence Sejnowski realiz&#243; modelos matem&#225;ticos y biolOgicos, con Geo_ Hinton realizaron el algoritmo de la mAquina de, y su extensi&#243;n a mayor orden, siendo la primera red de neuronas artificial que reconoc&#237;a un algoritmo de aprendizaje para una red de tres capas. La m&#225;quina de Boltzmann se aplic&#243; en distintas Areas . Tambi&#233;n ha trabajado en el reconocimiento de voz con su contribuci&#243;n al algoritmo de Retropopagaci&#243;n. Los psicologos McClelland y Rumelhart trabajaron en modelos de redes de neuronas artificiales como herramienta en la compresi&#243;n de la mente. 
